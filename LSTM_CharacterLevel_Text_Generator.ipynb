{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwhCs9ZikFuuGmc5bDQQCM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nedu21/Pytorch-deep-learning-projects-/blob/main/LSTM_CharacterLevel_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"The Ghost in the Shell\" (Character-Level Text Generator)"
      ],
      "metadata": {
        "id": "fTApRzvCEL5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hkvfjAhnRznH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Creation & Pre-processing"
      ],
      "metadata": {
        "id": "dWVHj8PdEqbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Raw Data\n",
        "# We'll use a tiny snippet of Shakespeare for this experiment.\n",
        "text = \"\"\"\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep;\n",
        "To sleep, perchance to dream—ay, there's the rub:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zQAhG43TEW0u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Build the vocabulary\n",
        "# 'set(text)' finds unique chars, 'sorted' puts them in order\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f'Total Characters in text: {len(text)}')\n",
        "print(f'Unique Characters (Vocab Size): {vocab_size}')\n",
        "print(f'Vocabulary: {''.join(chars)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzFbHObGExFH",
        "outputId": "034d3098-2cbf-4bfb-e310-550660fe079c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters in text: 435\n",
            "Unique Characters (Vocab Size): 38\n",
            "Vocabulary: \n",
            " ',-.:;ADNOTWabcdefghiklmnopqrstuvwy—\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create Mappings\n",
        "# char_to_ix: Converts 'a' -> 14\n",
        "# ix_to_char: Converst 14 -> 'a'\n",
        "char_to_ix = { ch:i for i, ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars) }"
      ],
      "metadata": {
        "id": "k2mMHvEuKONe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Helper Function: String to Tensor\n",
        "def str_to_tensor(s):\n",
        "  \"\"\"\n",
        "    Converts a string like \"Hello\" into a LongTensor like [12, 5, 20, 20, 31]\n",
        "    unsqueeze(0) adds the batch dimension: Shape (1, Sequence_Length)\n",
        "  \"\"\"\n",
        "  idxs = [char_to_ix[c] for c in s]\n",
        "  return torch.tensor(idxs, dtype=torch.long).unsqueeze(0)"
      ],
      "metadata": {
        "id": "3WJXjRXlMfNf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Prepare Training data\n",
        "# Input: Everything EXCEPT the last char\n",
        "# Target: Everything EXCEPT the first char\n",
        "input_seq = str_to_tensor(text[:-1])\n",
        "target_seq = str_to_tensor(text[1:])\n",
        "\n",
        "print(f\"\\nInput Shape: {input_seq.shape}\")\n",
        "print(f\"Target Shape: {target_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhI5iF_FPTcd",
        "outputId": "de55773d-a007-4e16-f28a-88456bcd6dea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input Shape: torch.Size([1, 434])\n",
            "Target Shape: torch.Size([1, 434])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check\n",
        "print(f\"First 5 Input Chars:  {[ix_to_char[ix.item()] for ix in input_seq[0, :5]]}\")\n",
        "print(f\"First 5 Target Chars: {[ix_to_char[ix.item()] for ix in target_seq[0, :5]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vveMU-E2Uw5V",
        "outputId": "d19378aa-5805-4c27-f892-2bad74cf8b20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 Input Chars:  ['\\n', 'T', 'o', ' ', 'b']\n",
            "First 5 Target Chars: ['T', 'o', ' ', 'b', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation"
      ],
      "metadata": {
        "id": "FfB3oLeEhuU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTMCell(nn.Module):\n",
        "  def __init__(self, input_sz, hidden_sz):\n",
        "    super().__init__()\n",
        "    self.input_sz = input_sz\n",
        "    self.hidden_sz = hidden_sz\n",
        "    self.W = nn.Parameter(torch.Tensor(input_sz + hidden_sz, hidden_sz * 4))\n",
        "    self.b = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_sz)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, x, init_states=None):\n",
        "    h_prev, C_prev = init_states\n",
        "    combined = torch.cat((x, h_prev), 1)\n",
        "    gates = combined @ self.W + self.b\n",
        "    slices = gates.chunk(4, dim=1)\n",
        "\n",
        "    f_t = torch.sigmoid(slices[0])\n",
        "    i_t = torch.sigmoid(slices[1])\n",
        "    C_tilde = torch.tanh(slices[2])\n",
        "    o_t = torch.sigmoid(slices[3])\n",
        "\n",
        "    C_t = (f_t * C_prev) + (i_t * C_tilde)\n",
        "    h_t = o_t * torch.tanh(C_t)\n",
        "    return h_t, C_t"
      ],
      "metadata": {
        "id": "KzegHZwqYURl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "  def __init__(self, input_sz, hidden_sz):\n",
        "    super().__init__()\n",
        "    self.input_sz = input_sz\n",
        "    self.hidden_sz = hidden_sz\n",
        "    self.cell = CustomLSTMCell(input_sz, hidden_sz)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape: (batch_size, seq_len, input_sz)\n",
        "    batch_size, seq_len, _ = x.size()\n",
        "\n",
        "    # Initialize Memory (Zeros)\n",
        "    h_t = torch.zeros(batch_size, self.hidden_sz)\n",
        "    C_t = torch.zeros(batch_size, self.hidden_sz)\n",
        "\n",
        "    hidden_state = []\n",
        "\n",
        "    for t in range(seq_len):\n",
        "      x_t = x[:, t, :]\n",
        "      h_t, C_t = self.cell(x_t, (h_t, C_t))\n",
        "      hidden_state.append(h_t)\n",
        "\n",
        "    return torch.stack(hidden_state, dim=1)"
      ],
      "metadata": {
        "id": "OyFO9DfmjWEf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THE MODEL WRAPPER\n",
        "class TextGenerator(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    # 1. Embedding: Map index -> vector\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. Your Engine: vector -> hidden memory\n",
        "    self.lstm = CustomLSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    # 3. Decoder: hidden memory -> vocab_size\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape: (batch, seq_len)\n",
        "\n",
        "    # Embed: (batch, seq_len, embed_dim)\n",
        "    embeds = self.embedding(x)\n",
        "\n",
        "    # Run LSTM: (batch, seq_len, hidden_dim)\n",
        "    lstm_out = self.lstm(embeds)\n",
        "\n",
        "    # Decode: (batch, seq_len, vocab_size)\n",
        "    logits = self.fc(lstm_out)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "vqZUoiOYnnqO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EMBED_DIM = 32\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "# Instantiate\n",
        "model = TextGenerator(vocab_size, EMBED_DIM, HIDDEN_DIM)\n",
        "\n",
        "# Dummy Pass (Use the input_seq from Step 1)\n",
        "dummy_output = model(input_seq)\n",
        "\n",
        "print(f\"Input Shape:  {input_seq.shape}\")\n",
        "print(f\"Output Shape: {dummy_output.shape}\")\n",
        "print(f\"Expected:     (1, {input_seq.shape[1]}, {vocab_size})\")\n",
        "\n",
        "if dummy_output.shape == (1, input_seq.shape[1], vocab_size):\n",
        "    print(\"SUCCESS: Dimensions match!\")\n",
        "else:\n",
        "    print(\"FAILURE: Dimension mismatch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2K_untKpatD",
        "outputId": "b7b95737-3a9d-4442-dea8-74a3bfe7bb6f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape:  torch.Size([1, 434])\n",
            "Output Shape: torch.Size([1, 434, 38])\n",
            "Expected:     (1, 434, 38)\n",
            "SUCCESS: Dimensions match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "fNL-uI06v2jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "LR = 0.01\n",
        "EPOCHS = 500\n",
        "\n",
        "# Setup tools\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "print(f'Starting training on {len(text)} characters...')\n",
        "\n",
        "# The loop\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  logits = model(input_seq)\n",
        "  # Calculate Loss\n",
        "  # We must FLATTEN the output and target for CrossEntropy\n",
        "  # View(-1, VOCAB_SIZE) -> Stacks all time steps on top of each other\n",
        "  # View(-1) -> Flattens the target into one long list of indices\n",
        "  loss = criterion(logits.view(-1, vocab_size), target_seq.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss_history.append(loss.item())\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(f'Epoch {epoch} | Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22x79NrcpeEC",
        "outputId": "d226f405-4e5d-4bb9-a8ae-a68f9f751869"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on 435 characters...\n",
            "Epoch 0 | Loss: 3.6247\n",
            "Epoch 50 | Loss: 0.5412\n",
            "Epoch 100 | Loss: 0.0319\n",
            "Epoch 150 | Loss: 0.0120\n",
            "Epoch 200 | Loss: 0.0069\n",
            "Epoch 250 | Loss: 0.0046\n",
            "Epoch 300 | Loss: 0.0746\n",
            "Epoch 350 | Loss: 0.0128\n",
            "Epoch 400 | Loss: 0.0073\n",
            "Epoch 450 | Loss: 0.0050\n",
            "Final Loss: 0.0038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation"
      ],
      "metadata": {
        "id": "JzfT8_N60aVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, start_str=\"To be\", predict_len=200, temperature=0.5):\n",
        "    \"\"\"\n",
        "    temperature:\n",
        "       < 1.0 (Conservative, rigid, sticks to training data)\n",
        "       > 1.0 (Creative, chaotic, makes mistakes)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Prepare Initial Input\n",
        "    current_input = str_to_tensor(start_str)\n",
        "    generated_text = start_str\n",
        "\n",
        "    print(f\"--- GENERATING (Temp: {temperature}) ---\")\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        with torch.no_grad():\n",
        "            # A. Get logits\n",
        "            output = model(current_input)\n",
        "\n",
        "            # B. Look at ONLY the last character's prediction\n",
        "            last_char_logits = output[0, -1, :]\n",
        "\n",
        "            # C. Apply Temperature & Softmax\n",
        "            # Dividing by temp flattens or sharpens the curve\n",
        "            probs = F.softmax(last_char_logits / temperature, dim=0)\n",
        "\n",
        "            # D. Sample from the distribution\n",
        "            predicted_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # E. Decode and Append\n",
        "            generated_char = ix_to_char[predicted_idx]\n",
        "            generated_text += generated_char\n",
        "\n",
        "            # F. Update Input for next step\n",
        "            # We append the new index to the sequence\n",
        "            next_idx_tensor = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
        "            current_input = torch.cat([current_input, next_idx_tensor], dim=1)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# --- RUN IT ---\n",
        "# Since your loss is near 0, a low temp (0.5) should recite Shakespeare perfectly.\n",
        "print(generate_text(model, start_str=\"To be\", predict_len=300, temperature=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUua2We9za49",
        "outputId": "cb885671-e00a-48bf-fb59-e479a2fd2a7a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GENERATING (Temp: 0.5) ---\n",
            "To be, or not to be, that is the question:\n",
            "Whether 'tis nobler in the mind to suffer\n",
            "The slings and arrows of outrageous fortune,\n",
            "Or to take arms against a sea of troubles\n",
            "And by opposing end them. To die—to sleep,\n",
            "No more; and by a sleep to say we end\n",
            "The heart-ache and the thousand natural shocks\n",
            "That \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Zb1IREh0qHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}