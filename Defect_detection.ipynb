{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i_3t8ju4imAGJkfCXvwyYkl2CbzstY7W",
      "authorship_tag": "ABX9TyPoBK16IxpiBsSHaEqt6lut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nedu21/Pytorch-deep-learning-projects-/blob/main/Defect_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJYhWKdGnkQm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring repeatablility across runs in PyTorch for consistent results.\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
        "    make cuDNN run in deterministic mode to ensure reproducibility.\"\"\"\n",
        "\n",
        "    # ---- Seed Python's built-in random module -----------------------\n",
        "    random.seed(seed);\n",
        "    # ---- Seed NumPy's random number generator -----------------------\n",
        "    np.random.seed(seed);\n",
        "\n",
        "    # ---- Seed PyTorch (CPU & all GPUs) ------------------------------\n",
        "    torch.manual_seed(seed);            # Seed for CPU operations\n",
        "    torch.cuda.manual_seed_all(seed);   # Seed for all GPU operations\n",
        "\n",
        "    # ---- cuDNN: Configure for repeatable convolutions ---------------\n",
        "    # This ensures that cudnn algorithms are deterministic.\n",
        "    torch.backends.cudnn.deterministic = True;\n",
        "    # Disable cuDNN benchmarking to ensure consistent execution speed (can be slower).\n",
        "    torch.backends.cudnn.benchmark     = False;\n",
        "\n",
        "# Global seed for all random operations.\n",
        "SEED = 42;\n",
        "# Apply the global seed to all relevant libraries.\n",
        "set_seed(SEED);\n",
        "print(f\"Global seed set to {SEED} — main process is now deterministic.\");\n",
        "\n",
        "# Define worker_init_fn function for DataLoader workers.\n",
        "def worker_init_fn(worker_id: int) -> None:\n",
        "    \"\"\"Re-seed each DataLoader worker so their RNGs don't collide.\n",
        "    This ensures that each worker gets a unique, but reproducible, sequence of random numbers.\"\"\"\n",
        "    worker_seed = SEED + worker_id;\n",
        "    np.random.seed(worker_seed);\n",
        "    random.seed(worker_seed);\n",
        "    torch.manual_seed(worker_seed);\n",
        "\n",
        "# Create a Generator object to manage PyTorch's internal randomness in DataLoaders.\n",
        "g = torch.Generator();\n",
        "# Set the seed for the generator to ensure reproducibility of DataLoader shuffling and transformations.\n",
        "g.manual_seed(SEED);"
      ],
      "metadata": {
        "id": "SKXjbVFsA1Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0719982b-7984-4236-9b72-9fb70fd5ca2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 42 — main process is now deterministic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script_dir = os.getcwd()\n",
        "folder_path = os.path.join(script_dir, 'drive', 'MyDrive', 'NEU-DET')\n",
        "\n",
        "train_dir = os.path.join(folder_path, 'train')\n",
        "train_img_dir = os.path.join(train_dir, 'images')\n",
        "\n",
        "val_dir = os.path.join(folder_path, 'validation')\n",
        "val_img_dir = os.path.join(val_dir, 'images')"
      ],
      "metadata": {
        "id": "CAIN3tGqt8id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(os.listdir(val_img_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1daAwXd2zB-l",
        "outputId": "2d42335f-1e7d-4bf5-ee1c-b86be0ec1c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crazing',\n",
              " 'inclusion',\n",
              " 'patches',\n",
              " 'pitted_surface',\n",
              " 'rolled-in_scale',\n",
              " 'scratches']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NEUDataset(Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    # Get the class names\n",
        "    self.classes = sorted(os.listdir(img_dir))\n",
        "    # Create a mapping: {'crazing': 0, 'inclusion': 1, ......}\n",
        "    self.class_to_idx = {name: i for i, name in enumerate(self.classes)}\n",
        "\n",
        "    # Create a list of every single image path and its label\n",
        "    self.images = []\n",
        "    for class_name in self.classes:\n",
        "      class_dir = os.path.join(img_dir, class_name)\n",
        "      for img_name in os.listdir(class_dir):\n",
        "        # Store the path to image & its numerical label\n",
        "        self.images.append((os.path.join(class_dir, img_name), self.class_to_idx[class_name]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 1. Look up the address and label in our master list\n",
        "    img_path, label = self.images[idx]\n",
        "\n",
        "    # 2. Open the img file\n",
        "    # We use .convert('RGB') to ensure img have 3 channels (Some may be grayscale but models usually expect 3 channels)\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "idmMMpBcoiXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5042, 0.5042, 0.5042], std=[0.2058, 0.2058, 0.2058])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5042, 0.5042, 0.5042], std=[0.2058, 0.2058, 0.2058])\n",
        "])\n",
        "\n",
        "train_data = NEUDataset(img_dir=train_img_dir, transform=train_transforms)\n",
        "val_data = NEUDataset(img_dir=val_img_dir, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    worker_init_fn=worker_init_fn,\n",
        "    generator=g\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    worker_init_fn=worker_init_fn,\n",
        "    generator=g\n",
        ")"
      ],
      "metadata": {
        "id": "AJu8O73F8Pj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model building"
      ],
      "metadata": {
        "id": "subUHeQ1PSno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, out_channel_1=16, out_channel_2=32, out_channel_3=64, out_channel_4=128):\n",
        "    super().__init__()\n",
        "    # Conv layers\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=out_channel_1, kernel_size=3, stride=1, padding='same'),\n",
        "        nn.BatchNorm2d(out_channel_1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=out_channel_1, out_channels=out_channel_2, kernel_size=3, stride=1, padding='same'),\n",
        "        nn.BatchNorm2d(out_channel_2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=out_channel_2, out_channels=out_channel_3, kernel_size=3, stride=1, padding='same'),\n",
        "        nn.BatchNorm2d(out_channel_3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=out_channel_3, out_channels=out_channel_4, kernel_size=3, stride=1, padding='same'),\n",
        "        nn.BatchNorm2d(out_channel_4),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(out_channel_4*14*14, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, 6)\n",
        "    )\n",
        "\n",
        "    # Apply Kaiming Initialization\n",
        "    self._initialize_weights()\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "        # Apply Kaiming to all Convolutional layers\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            # 'fan_out' preserves the magnitude in the backward pass\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # BatchNorm layers start with weight 1 and bias 0\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Linear layers use small random weights\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_layers(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "IG_aIEGtORIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()"
      ],
      "metadata": {
        "id": "fpK2xsytlYOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Training Configuration"
      ],
      "metadata": {
        "id": "Puu54lneYXoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)"
      ],
      "metadata": {
        "id": "HKGDU8etX6wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "sANugufa0T-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []"
      ],
      "metadata": {
        "id": "jbM_8U2r0zEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, criterion, optimizer):\n",
        "  dataset_size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  train_loss, train_correct = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    yhat = model(X)\n",
        "    loss = criterion(yhat, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss += loss.item() * len(X)\n",
        "    # argmax(1) finds the index of the highest score (the predicted class)\n",
        "    train_correct += (yhat.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  avg_train_loss = train_loss / dataset_size\n",
        "  avg_train_acc = train_correct / dataset_size\n",
        "\n",
        "  train_loss_history.append(avg_train_loss)\n",
        "  train_acc_history.append(avg_train_acc)\n",
        "\n",
        "  print(f\"Train Error: Accuracy: {100*avg_train_acc:>0.1f}%, Avg Loss: {avg_train_loss:>7f}\")"
      ],
      "metadata": {
        "id": "EIo4FMxAyyKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, criterion):\n",
        "  dataset_size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  test_loss, test_correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      yhat = model(X)\n",
        "      loss = criterion(yhat, y)\n",
        "\n",
        "      test_loss += loss.item() * len(X)\n",
        "      test_correct += (yhat.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  avg_test_loss = test_loss / dataset_size\n",
        "  avg_test_acc = test_correct / dataset_size\n",
        "\n",
        "  val_loss_history.append(avg_test_loss)\n",
        "  val_acc_history.append(avg_test_acc)\n",
        "\n",
        "  print(f\"Test Error:  Accuracy: {100*avg_test_acc:>0.1f}%, Avg Loss: {avg_test_loss:>7f}\\n\")\n",
        "  return avg_test_loss"
      ],
      "metadata": {
        "id": "7qhs9zq32zhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
        "    train(train_loader, model, criterion, optimizer)\n",
        "    val_loss = test(val_loader, model, criterion)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "\n",
        "        torch.save(model.state_dict(), 'best_neu_model.pth')\n",
        "        print(\"Saved best model!\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZM_6OiWZfX4",
        "outputId": "b973bb0c-6644-410c-d638-ddfca7a892aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 63.6%, Avg Loss: 0.945481\n",
            "Test Error:  Accuracy: 77.5%, Avg Loss: 0.647818\n",
            "\n",
            "Saved best model!\n",
            "Epoch 2/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 82.5%, Avg Loss: 0.511130\n",
            "Test Error:  Accuracy: 80.3%, Avg Loss: 0.489778\n",
            "\n",
            "Saved best model!\n",
            "Epoch 3/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 89.0%, Avg Loss: 0.361864\n",
            "Test Error:  Accuracy: 83.1%, Avg Loss: 0.411821\n",
            "\n",
            "Saved best model!\n",
            "Epoch 4/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 88.7%, Avg Loss: 0.355513\n",
            "Test Error:  Accuracy: 39.7%, Avg Loss: 2.947612\n",
            "\n",
            "Epoch 5/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 88.3%, Avg Loss: 0.402859\n",
            "Test Error:  Accuracy: 88.9%, Avg Loss: 0.398000\n",
            "\n",
            "Saved best model!\n",
            "Epoch 6/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 88.9%, Avg Loss: 0.327904\n",
            "Test Error:  Accuracy: 75.3%, Avg Loss: 0.667418\n",
            "\n",
            "Epoch 7/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 90.1%, Avg Loss: 0.336359\n",
            "Test Error:  Accuracy: 86.4%, Avg Loss: 0.390657\n",
            "\n",
            "Saved best model!\n",
            "Epoch 8/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 93.1%, Avg Loss: 0.214814\n",
            "Test Error:  Accuracy: 85.0%, Avg Loss: 0.530071\n",
            "\n",
            "Epoch 9/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 89.8%, Avg Loss: 0.286749\n",
            "Test Error:  Accuracy: 86.9%, Avg Loss: 0.339510\n",
            "\n",
            "Saved best model!\n",
            "Epoch 10/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 92.3%, Avg Loss: 0.247588\n",
            "Test Error:  Accuracy: 91.1%, Avg Loss: 0.242134\n",
            "\n",
            "Saved best model!\n",
            "Epoch 11/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 91.0%, Avg Loss: 0.294803\n",
            "Test Error:  Accuracy: 85.3%, Avg Loss: 0.626070\n",
            "\n",
            "Epoch 12/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 92.0%, Avg Loss: 0.270421\n",
            "Test Error:  Accuracy: 89.2%, Avg Loss: 0.330444\n",
            "\n",
            "Epoch 13/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 94.4%, Avg Loss: 0.196856\n",
            "Test Error:  Accuracy: 89.7%, Avg Loss: 0.301387\n",
            "\n",
            "Epoch 14/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 93.0%, Avg Loss: 0.216916\n",
            "Test Error:  Accuracy: 89.4%, Avg Loss: 0.284220\n",
            "\n",
            "Epoch 15/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 94.6%, Avg Loss: 0.167145\n",
            "Test Error:  Accuracy: 92.8%, Avg Loss: 0.169811\n",
            "\n",
            "Saved best model!\n",
            "Epoch 16/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.8%, Avg Loss: 0.132296\n",
            "Test Error:  Accuracy: 93.9%, Avg Loss: 0.146682\n",
            "\n",
            "Saved best model!\n",
            "Epoch 17/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 96.3%, Avg Loss: 0.126604\n",
            "Test Error:  Accuracy: 95.6%, Avg Loss: 0.120436\n",
            "\n",
            "Saved best model!\n",
            "Epoch 18/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 96.5%, Avg Loss: 0.108294\n",
            "Test Error:  Accuracy: 92.5%, Avg Loss: 0.150249\n",
            "\n",
            "Epoch 19/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.3%, Avg Loss: 0.142538\n",
            "Test Error:  Accuracy: 92.8%, Avg Loss: 0.149776\n",
            "\n",
            "Epoch 20/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.8%, Avg Loss: 0.127151\n",
            "Test Error:  Accuracy: 93.9%, Avg Loss: 0.117512\n",
            "\n",
            "Saved best model!\n",
            "Epoch 21/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.8%, Avg Loss: 0.115174\n",
            "Test Error:  Accuracy: 91.9%, Avg Loss: 0.179196\n",
            "\n",
            "Epoch 22/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.3%, Avg Loss: 0.159146\n",
            "Test Error:  Accuracy: 93.9%, Avg Loss: 0.143476\n",
            "\n",
            "Epoch 23/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 95.7%, Avg Loss: 0.128537\n",
            "Test Error:  Accuracy: 94.4%, Avg Loss: 0.119034\n",
            "\n",
            "Epoch 24/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 96.3%, Avg Loss: 0.110611\n",
            "Test Error:  Accuracy: 92.5%, Avg Loss: 0.144560\n",
            "\n",
            "Epoch 25/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 97.1%, Avg Loss: 0.088476\n",
            "Test Error:  Accuracy: 95.0%, Avg Loss: 0.096448\n",
            "\n",
            "Saved best model!\n",
            "Epoch 26/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 97.3%, Avg Loss: 0.087721\n",
            "Test Error:  Accuracy: 95.3%, Avg Loss: 0.109942\n",
            "\n",
            "Epoch 27/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 97.4%, Avg Loss: 0.076401\n",
            "Test Error:  Accuracy: 95.6%, Avg Loss: 0.093822\n",
            "\n",
            "Saved best model!\n",
            "Epoch 28/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 97.4%, Avg Loss: 0.072083\n",
            "Test Error:  Accuracy: 95.8%, Avg Loss: 0.089559\n",
            "\n",
            "Saved best model!\n",
            "Epoch 29/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 96.9%, Avg Loss: 0.096650\n",
            "Test Error:  Accuracy: 95.6%, Avg Loss: 0.081955\n",
            "\n",
            "Saved best model!\n",
            "Epoch 30/30\n",
            "-------------------------------\n",
            "Train Error: Accuracy: 97.3%, Avg Loss: 0.066118\n",
            "Test Error:  Accuracy: 92.8%, Avg Loss: 0.151934\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}